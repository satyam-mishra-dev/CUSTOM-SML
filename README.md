ğŸš€ **I just built a functional mini-LLM from scratch!** ğŸš€  
Diving deep into the nuts and bolts of transformer architecture, I created a lightweight GPT-style language model designed for educational exploration and rapid prototyping. Hereâ€™s what makes it special:  

### ğŸ”§ **Core Highlights**  
- **Architecture**: Decoder-only transformer (6 layers, 6 attention heads, 384-dim embeddings) with a **128-token context window**.  
- **Tokenizer**: GPT-2 Byte Pair Encoding (BPE) via `tiktoken` (80k vocabulary).  
- **Training**: Optimized on the `TinyStories` dataset using **AdamW**, **gradient clipping (0.5)**, and **mixed precision (AMP)** for stability.  
- **Capabilities**: Generates coherent story snippets, handles simple prompts, and supports **temperature/top-k sampling** for creativity control.  
- **Efficiency**: Runs on a single GPUâ€”perfect for testing ideas fast!  

### ğŸ§  **Why This Matters**  
This project demystifies LLM internals while emphasizing:  
1ï¸âƒ£ **Scalability**: Even tiny transformers (6L/6H) can produce logical text.  
2ï¸âƒ£ **Training Tricks**: LR warmup + cosine decay, gradient accumulation, and AMP prevent instability.  
3ï¸âƒ£ **Real-World Gaps**: No safety layers yet (future work!), domain-limited to `TinyStories`.  

### ğŸŒŸ **Whatâ€™s Next?**  
- Integrate **RoPE/ALIBI** for longer contexts â†’ 128 tokens â¡ï¸ 512+.  
- Add **safety adapters** and **conversational fine-tuning**.  
- Publish a full research write-up!  

**ğŸ‘‰ Check out the [PDF] and [CODE] attached!**  
Iâ€™ve open-sourced everything to help others learnâ€”ideal for hands-on LLM newbies or educators. Feedback welcome!  

### ğŸ· **Top Tags**  
#AI #MachineLearning #DeepLearning #LLM #Transformers #NLP #ArtificialIntelligence #DataScience #NeuralNetworks #Python #OpenSource #TechRecruitment #BuildInPublic #LearnAI  


**Credits**: Made with inspiration from Andrej Karpathyâ€™s nanoGPT & TinyStories dataset.
